# GPUì—ì„œ Executors êµ¬ë™í•˜ê¸°

```{article-info}
:avatar: avatars/tadej.jpg
:avatar-link: https://jobs.jina.ai
:avatar-outline: muted
:author: Tadej @ Jina AI
:date: Sept. 1, 2021
```

ì´ íŠœí† ë¦¬ì–¼ì€ ë¡œì»¬ ë° ë„ì»¤ ì»¨í…Œì´ë„ˆ ëª¨ë‘ì—ì„œ, GPUì˜ Executorì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
ë˜í•œ ë¯¸ë¦¬ êµ¬ì¶•ëœ í—ˆë¸Œ executorì—ì„œ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œë„ ë°°ìš¸ ì˜ˆì •ì…ë‹ˆë‹¤.

GPUë¥¼ ì‚¬ìš©í•˜ë©´ ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ëŒ€í•´ ì¸ì½”ë”© ì†ë„ë¥¼ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆìœ¼ë©°, ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ê³¼ ì…ë ¥ ê°’ì— ë”°ë¼ ì‘ë‹µ ëŒ€ê¸°ì‹œê°„ì„ 5ë°°ì—ì„œ 100ë°°ê¹Œì§€ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

JinaëŠ” ë‹¹ì‹ ì´ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ì—ì„œë‚˜ ë„ì»¤ ì»¨í…Œì´ë„ˆì—ì„œ ê·¸ë¬ë“¯ì´ GPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤ - ì¶”ê°€ ìš”êµ¬ì‚¬í•­ì´ë‚˜ êµ¬ì„±ì„ ë¶€ê³¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```{admonition} Important
:class: important

This tutorial assumes you are already familiar with basic Jina concepts, such as Document, Executor, and Flow. Some knowledge of the [Hub](../advanced/hub/index) is also needed for the last part of the tutorial.

If you're not yet familiar with these concepts, first read the [Basic Concepts](../fundamentals/concepts) and related documentation, and return to this tutorial once you feel comfortable performing baisc operations in Jina.
```

## ì „ì œì¡°ê±´

ì´ íŠœí† ë¦¬ì–¼ì€ NVIDIA ê·¸ë˜í”½ ì¹´ë“œê°€ ìˆëŠ” ì»´í“¨í„°ì—ì„œ ì‘ì—…í•´ì•¼í•©ë‹ˆë‹¤. ì§‘ì— ì´ëŸ¬í•œ ê¸°ê¸°ê°€ ì—†ëŠ” ê²½ìš° ë‹¤ì–‘í•œ ë¬´ë£Œ í´ë¼ìš°ë“œ í”Œë«í¼ (ì˜ˆ: Google Colab ë˜ëŠ” Kaggle ì»¤ë„)ì„ ì‚¬ìš© í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ ìµœì‹  ë²„ì ¼ì˜ [NVIDIA ë“œë¼ì´ë²„](https://www.nvidia.com/Download/index.aspx)ë¥¼ ì„¤ì¹˜í•´ì•¼í•©ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ëŠ” CUDAë¥¼ ì„¤ì¹˜í•  í•„ìš”ê°€ ì—†ì§€ë§Œ, ì‚¬ìš©í•˜ëŠ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì— ë”°ë¼ ë¡œì»¬ ì‹¤í–‰ì— í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

íŠœí† ë¦¬ì–¼ì˜ ë„ì»¤ ë¶€ë¶„ì—ì„œëŠ” [ë„ì»¤](https://docs.docker.com/get-docker/) ì™€ 
[nvidia-ë„ì»¤](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) ë¥¼ ì„¤ì¹˜í•´ì•¼í•©ë‹ˆë‹¤.

íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ê°€ìƒí™˜ê²½ (ì˜ˆ: [venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environment) ë˜ëŠ” [conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html#managing-environments))ì´ í•„ìš”í•˜ë©° ë‹¤ìŒì„ ì‚¬ìš©í•˜ì—¬ Jinaë¥¼ ì„¤ì¹˜í•´ì•¼í•©ë‹ˆë‹¤.

```bash
pip install jina
```

## executor ì„¸íŒ…í•˜ê¸°

ê°„ë‹¨í•œ ë¬¸ì¥ ì¸ì½”ë”ë¥¼ ë§Œë“¤ê³  Jinaì˜ ëª…ë ¹í–‰ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ì—¬ "skeleton"ì„ ë§Œë“œëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤:

```bash
jina hub new
```

ì…ë ¥ì„ ìš”ì²­í•˜ë©´, ì¸ì½”ë” ì´ë¦„ì„ `SentenceEncoder`ë¡œ ì§€ì •í•˜ê³  ê¸°ë³¸ í´ë”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ - í˜„ì¬ ë””ë ‰í† ë¦¬ ë‚´ì— `SentenceEncoder/` í´ë”ê°€ ìƒì„±ë˜ë©°, ì´ ë””ë ‰í† ë¦¬ëŠ” íŠœí† ë¦¬ì–¼ì˜ ì‘ì—… ë””ë ‰í† ë¦¬ê°€ ë©ë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ, ê³ ê¸‰ ì„¤ì •ì„ ë¬»ëŠ” ë©”ì‹œì§€ê°€ ë‚˜íƒ€ë‚˜ë©´ `y` ë¥¼ ì„ íƒí•˜ê³  ë„ì»¤ íŒŒì¼ì„ ë§Œë“¤ ê²ƒì¸ì§€ ë¬»ëŠ” ë©”ì‹œì§€ê°€ í‘œì‹œë  ë•Œë¥¼ ì œì™¸í•˜ê³  ë‹¤ë¥¸ ëª¨ë“  ì§ˆë¬¸ì€ ë¹„ì›Œ ë‘¡ë‹ˆë‹¤ - ì´ ì§ˆë¬¸ì— `y`ë¡œ ë‹µí•˜ì„¸ìš”(ë‹¤ìŒ ì„¹ì…˜ì—ì„œ í•„ìš”í•©ë‹ˆë‹¤). ì´ê²ƒì´ ì´ ëŒ€í™”ê°€ ë§ˆì§€ë§‰ì— ì–´ë–»ê²Œ ë³´ì—¬ì ¸ì•¼ í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.

![jina hub new](../_static/hub_new_gpu.png)


ë‹¤ ëë‚˜ë©´ ìƒˆë¡œ ë§Œë“¤ì–´ì§„ Executor ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•©ë‹ˆë‹¤:
```bash
cd SentenceEncoder
```

ê³„ì†í•´ì„œ `requirements.txt` íŒŒì¼ì˜ ìš”êµ¬ ì‚¬í•­ë“¤ì„ ì§€ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```text
sentence-transformers==2.0.0
```

í•˜ë‹¨ì„ ì°¸ê³ í•˜ì—¬ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```bash
pip install -r requirements.txt
```

```{admonition} Do I need to install CUDA?
:class: info

All machine learning frameworks rely on CUDA for running on GPU. However, whether you
need CUDA installed on your system or not depends on the framework that you are using.

In this tutorial, we are using PyTorch framework, which already includes the necessary
CUDA binaries in its distribution. However, other frameworks, such as Tensorflow, require
you to install CUDA yourself.
```

```{admonition} Install only what you need
:class: tip

In this example we are installing the GPU-enabled version of PyTorch, which is the default
version when installing from PyPI. However, if you know that you only need to use your
executor on CPU, you can save a lot of space (100s of MBs, or even GBs) by installing
CPU-only versions of your requirements. This translates into faster start-up times
when using Docker containers.

In our case, we could change the `requirements.txt` file to install a CPU-only version
of PyTorch like this

:::text
-f https://download.pytorch.org/whl/torch_stable.html
sentence-transformers
torch==1.9.0+cpu
:::
```

ì´ì œ `executor.py` íŒŒì¼ì„ Executorì˜ ì‹¤ ì½”ë“œë¡œ ì±„ì›Œ ë„£ìì‹œë‹¤.

```{code-block} python
---
emphasize-lines: 16, 17
---
from typing import Optional

import torch
from jina import DocumentArray, Executor, requests
from sentence_transformers import SentenceTransformer


class SentenceEncoder(Executor):
    """A simple sentence encoder that can be run on a CPU or a GPU

    :param device: The pytorch device that the model is on, e.g. 'cpu', 'cuda', 'cuda:1'
    """

    def __init__(self, device: str = 'cpu', *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
        self.model.to(device)  # Move the model to device

    @requests
    def encode(self, docs: Optional[DocumentArray], **kwargs):
        """Add text-based embeddings to all documents"""
        texts = docs.get_attributes("text")
        with torch.no_grad():
            embeddings = self.model.encode(texts, batch_size=32)

        for doc, embedding in zip(docs, embeddings):
            doc.embedding = embedding
```

ì—¬ê¸°ì„œ ëª¨ë“  ì¥ì¹˜ë³„ ë§ˆë²•ì€ ë‘ê°œì˜ ê°•ì¡° í‘œì‹œëœ ë¼ì¸ì—ì„œ ë°œìƒí•©ë‹ˆë‹¤ - `SentenceEncoder` í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ ë•Œ ì¥ì¹˜ë¥¼ ì „ë‹¬í•œ ë‹¤ìŒ, PyTorch ëª¨ë¸ì„ ì¥ì¹˜ë¡œ ì´ë™í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¨ê³„ëŠ” ë…ë¦½ ì‹¤í–‰í˜• íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒê³¼ ë™ì¼í•©ë‹ˆë‹¤.

Executorê°€ ì‚¬ìš©í•˜ê¸¸ ì›í•˜ëŠ” ì¥ì¹˜ë¥¼ ì–´ë–»ê²Œ ì „ë‹¬í• ì§€ ë³´ê¸° ìœ„í•´ , 10000ê°œì˜ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì¸ì½”ë”© í•˜ì—¬ ì´ ì¸ì½”ë”ì˜ ì‚¬ìš©ë²•ì„ ë³´ì—¬ì¤„ ë‹¤ë¥¸ íŒŒì¼ì¸ `main.py`ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.
 
```python
from jina import Document, Flow

from executor import SentenceEncoder

def generate_docs():
    for _ in range(10_000):
        yield Document(
            text='Using a GPU allows you to significantly speed up encoding.'
        )

f = Flow().add(uses=SentenceEncoder, uses_with={'device': 'cpu'})
with f:
    f.post(on='/encode', inputs=generate_docs, show_progress=True, request_size=32)
```

ì´ì œ ì‹¤í–‰í•´ë´…ì‹œë‹¤.

```bash
python main.py
```
```console
      executor0@26554[L]:ready and listening
        gateway@26554[L]:ready and listening
           Flow@26554[I]:ğŸ‰ Flow is ready to use!
        ğŸ”— Protocol:            GRPC
        ğŸ  Local access:        0.0.0.0:56969
        ğŸ”’ Private network:     172.31.39.70:56969
        ğŸŒ Public address:      52.59.231.246:56969
Working... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”â” 0:00:22 13.8 step/s 314 steps done in 22 seconds
```

## GPU ë¡œì»¬ë¡œ ì‚¬ìš©í•˜ê¸°

ì´ì œ GPUì—ì„œ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì–¼ë§ˆë‚˜ ì‰¬ìš´ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤ - ê°„ë‹¨í•œ ì¥ì¹˜ë¥¼ ì´ˆê¸°í™” í•  ë•Œ `'cuda'` ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

```diff
+ f = Flow().add(uses=SentenceEncoder, uses_with={'device': 'cuda'})
- f = Flow().add(uses=SentenceEncoder, uses_with={'device': 'cpu'})
```

CPUì— ë¹„í•´ GPUê°€ ì–¼ë§ˆë‚˜ ë¹ ë¥¸ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë¹„êµëŠ” ë‹¨ì¼ NVIDIA T4 GPUê°€ ì—°ê²°ëœ `g4dn.xlarge` AWS ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

ì²«ì§¸, ì¸ì½”ë”ê°€ GPUë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤ - ìœ„ì™€ ê°™ì´ `main.py`ì—ì„œ `'device'` íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì„ ì™„ë£Œí•˜ê³  ë²¤ì¹˜ë§ˆí¬ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
python main.py
```
```console
      executor0@21032[L]:ready and listening
        gateway@21032[L]:ready and listening
           Flow@21032[I]:ğŸ‰ Flow is ready to use!
        ğŸ”— Protocol:            GRPC
        ğŸ  Local access:        0.0.0.0:54255
        ğŸ”’ Private network:     172.31.39.70:54255
        ğŸŒ Public address:      52.59.231.246:54255
Working... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”â” 0:00:02 104.9 step/s 314 steps done in 2 seconds
```

7ë°° ì´ìƒ ë¹¨ë¼ì§„ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ê·¸ëŸ¬ë‚˜ ì´ëŠ” ìš°ë¦¬ê°€ í•  ìˆ˜ ìˆëŠ” ìµœì„ ì˜ ë°©ë²•ì´ ì•„ë‹™ë‹ˆë‹¤. batch í¬ê¸°ë¥¼ ëŠ˜ë ¤ GPUì˜ ë©”ëª¨ë¦¬ë¥¼ ìµœëŒ€í™” í•˜ë©´ í›¨ì”¬ ë” ë¹ ë¥¸ ì†ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ìµœì í™”ëŠ” ì´ íŠœí† ë¦¬ì–¼ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.

```{admonition} Note
:class: note

You have probably noticed that there was a delay (about 3 seconds) when creating the Flow.
This occured because the weights of our model needed to be transfered from CPU to GPU when we
initialized the Executor. However, this action only occurs once in the lifetime of the Executor,
so for most use cases this is not something we would worry about.
```

## ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì˜ GPU ì‚¬ìš©í•˜ê¸°

í”„ë¡œë•ì…˜ì—ì„œ Executorë¥¼ ì‚¬ìš©í•  ê²½ìš° ì ì ˆí•œ í™˜ê²½ ê²©ë¦¬ë¥¼ ì œê³µí•˜ê³  ëª¨ë“  ì¥ì¹˜ì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì»¤ ì»¨í…Œì´ë„ˆì— Executorë¥¼ ë„£ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ì´ ê²½ìš° GPU-ì‚¬ìš© Executorë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ì–´ë µì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ê²½ìš° ê¸°ë³¸ ë„ì»¤ íŒŒì¼ì„ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

```{admonition} Choosing the right base image

In our case we are using the default `jinaai/jina:latest` base image. However, parallel to the comments about having to install CUDA locally, you might need to use a different base image, depending on the framework you are using.

If you need to have CUDA installed in the image, you usually have two options: either you take the `nvidia/cuda` for the base image, or you take the official GPU-enabled image of the framework you are using, for example, `tensorflow/tensorflow:2.6.0-gpu`.
```

ìš°ë¦¬ê°€ ì‹ ê²½ ì“°ëŠ” ë‹¤ë¥¸ íŒŒì¼ì€ `config.yml`ì´ë©°, ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ ë²„ì „ë„ ì‘ë™í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‹ˆ ë„ì»¤ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤.

```bash
docker build -t sentence-encoder .
```

ëª¨ë“  ê²ƒì´ ì˜ ì‘ë™í•˜ëŠ”ì§€ ì‹ ì†í•˜ê²Œ í™•ì¸í•˜ê¸° ìœ„í•´ ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
docker run sentence-encoder
```

ì´ì œ GPUì™€ í•¨ê»˜ ë„ì»¤ ë²„ì ¼ì˜ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì´ì „ì— ì»¨í…Œì´ë„ˆì— ìˆëŠ” GPUë¥¼ ë‹¤ë¤„ë³¸ ì ì´ ìˆë‹¤ë©´, ì»¨í…Œì´ë„ˆ ì•ˆì— GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `--gpus all` ì˜µì…˜ì„ `docker run` ì»¤ë©˜ë“œì—ì„œ ì‚¬ìš©í•´ì•¼í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  JinaëŠ” ë°”ë¡œ ê·¸ ì¼ì„ í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

GPU ê¸°ë°˜ ì»¨í…Œì´ë„ˆ Executorë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ `main.py` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìˆ˜ì •í•´ì•¼í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```{code-block} python
---
emphasize-lines: 12
---
from jina import Document, DocumentArray, Flow

from executor import SentenceEncoder

def generate_docs():
    for _ in range(10000):
        yield Document(
            text='Using a GPU enables you to significantly speed up encoding'
        )

f = Flow().add(
    uses='docker://sentence-encoder', uses_with={'device': 'cuda'}, gpus='all'
)
with f:
    f.post(on='/encode', inputs=generate_docs, show_progress=True, request_size=32)
```

`python main.py`ì—ì„œ ì‹¤í–‰í•˜ë©´, ì´ì „ê³¼ ë™ì¼í•œ ê²°ê³¼ ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë„ì»¤ ì»¨í…Œì´ë„ˆì—ì„œë„ í•´ë‹¹í•˜ëŠ” ê²°ê³¼ ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Executorë¥¼ ì‹œì‘í•  ë•Œë§ˆë‹¤ íŠ¸ë ŒìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ ë©ë‹ˆë‹¤. ì´ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì¸ì½”ë”ê°€ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì—ì„œ ëª¨ë¸ì„ ë””ìŠ¤í¬ì— ë¡œë“œ í•˜ê¸¸ ì›í•©ë‹ˆë‹¤.

ìš°ë¦¬ëŠ” ë„ì»¤ ë³¼ë¥¨ìœ¼ë¡œ ì´ê²ƒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ - JinaëŠ” ê°„ë‹¨íˆ ê·¸ ì¸ìë¥¼ ë„ì»¤ ì»¨í…Œì´ë„ˆì— ì „ë‹¬ í•  ê²ƒì…ë‹ˆë‹¤.`main.py` ë¥¼ ìˆ˜ì •í•˜ì—¬ ì´ë¥¼ í—ˆìš© í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
f = Flow().add(
    uses='docker://sentence-encoder',
    uses_with={'device': 'cuda'},
    gpus='all',
    # This has to be an absolute path, replace /home/ubuntu with your home directory
    volumes="/home/ubuntu/.cache:/root/.cache",
)
```

ì—¬ê¸°ì„œëŠ” `~/.cache` ë””ë ‰í† ë¦¬ë¥¼ ë§ˆìš´íŠ¸ í–ˆìŠµë‹ˆë‹¤. ì´ ë””ë ‰í† ë¦¬ëŠ” ì‚¬ì „ êµ¬ì¶•ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ì €ì¥ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ë””ë ‰í† ë¦¬ëŠ” ì‚¬ìš©ì¤‘ì¸ íŒŒì´ì¬ íŒ¨í‚¤ì§€ì™€ ëª¨ë¸ ë¡œë“œ ê²½ë¡œ ì§€ì • ë°©ë²•ì— ë”°ë¼ ë‹¤ë¥¸ ë””ë ‰í† ë¦¬ë„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì´ì œ `python main.py` ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ë‹¤ìš´ë¡œë“œê°€ ì¼ì–´ë‚˜ì§€ ì•Šê³  ì¸ì½”ë”©ì´ ë” ë¹¨ë¦¬ ì‹œì‘ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## í—ˆë¸Œ Executorsì™€ GPU ì‚¬ìš©í•˜ê¸°

ì´ì œ ë¡œì»¬ì—ì„œ Executorì™€ ë„ì»¤ ì»¨í…Œì´ë„ˆì—ì„œ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤.Jina í—ˆë¸Œì—ì„œ Executorë¥¼ ì‚¬ìš©í•  ë•Œ ì°¨ì´ê°€ ìˆì—ˆë‚˜ìš”?

ì•„ë‹ˆìš”! Jina í—ˆë¸Œì˜ Executor ì¤‘ ë§ì€ ìˆ˜ê°€ GPU ì‚¬ìš© ë²„ì „ì´ ë¯¸ë¦¬ ë‚´ì¥ ë˜ì–´ ìˆìœ¼ë©°, ëŒ€ê°œ `gpu` íƒœê·¸ ( [Jina Hub tags] (hub_tags)ì°¸ê³  ) ì•„ë˜ì— ìˆìŠµë‹ˆë‹¤. ì˜ˆì œë¥¼ ìˆ˜ì •í•˜ì—¬ Jina í—ˆë¸Œì˜ ì‚¬ì „ ì œì‘ëœ `TransformerTorchEncoder` ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.

```diff
f = Flow().add(
-   uses='docker://sentence-encoder',
+   uses='jinahub+docker://TransformerTorchEncoder/gpu',
    uses_with={'device': 'cuda'},
    gpus='all',
    # This has to be an absolute path, replace /home/ubuntu with your home directory
    volumes="/home/ubuntu/.cache:/root/.cache",
)
```

ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì²˜ìŒ ì‹¤í–‰í•  ë•Œ ë„ì»¤ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ì†Œìš” ë©ë‹ˆë‹¤ - GPU ì´ë¯¸ì§€ê°€ í½ë‹ˆë‹¤! í•˜ì§€ë§Œ ê·¸ ì´í›„ì—ëŠ” ëª¨ë“  ê²ƒì´ ì—¬ëŸ¬ë¶„ì˜ ë¡œì»¬ ë„ì»¤ ì´ë¯¸ì§€ì—ì„œì²˜ëŸ¼ ì‘ë™í•©ë‹ˆë‹¤.

```{admonition} Important
:class: important

When using GPU encoders from Jina Hub, always use `jinahub+docker://`, and not `jinahub://`. As discussed above, these encoders might need CUDA installed (or other system dependencies), and installing that properly can be tricky. For that reason, you should prefer using Docker images, which already come with all these dependencies pre-installed.
```


## ê²°ë¡ 

ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ë³¸ ë‚´ìš©ì„ ë‹¤ì‹œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

1. GPUì—ì„œ ë¡œì»¬ë¡œ Executorì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë…ë¦½ ì‹¤í–‰í˜• ìŠ¤í¬ë¦½íŠ¸ì—ì„œ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë‹¤ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. Executorê°€ ì´ˆê¸°í™”ì— ì‚¬ìš©í•  ì¥ì¹˜ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. ë„ì»¤ ì»¨í…Œì´ë„ˆ ë‚´ì˜ GPUì—ì„œ Executorë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `gpus='all'` ì„ í†µê³¼ í•´ì•¼í•©ë‹ˆë‹¤.
3. ë³¼ë¥¨ ì‚¬ìš© ( ë°”ì¸ë“œ ë§ˆìš´íŠ¸ )ë¡œ Executorë¥¼ ì‹œì‘í•  ë•Œë§ˆë‹¤ ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
4. Jina í—ˆë¸Œì˜ Executorì™€ GPUë¥¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `gpu` íƒœê·¸ì™€ í•¨ê»˜ Executorë¥¼ ì‚¬ìš©í•´ì•¼í•©ë‹ˆë‹¤.

ë˜í•œ ì‚¬ìš©ì ê³ ìœ ì˜ Executor êµ¬ì¶•ì„ ì‹œì‘í•  ë•ŒëŠ” í•­ìƒ í•„ìš”í•œ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­(CUDA ë“±)ì„ í™•ì¸í•˜ê³  ê·¸ì— ë”°ë¼ ë¡œì»¬ (ë° ë„ì»¤ íŒŒì¼)ì— ì„¤ì¹˜ í•´ì•¼í•©ë‹ˆë‹¤.
