# Build a Streaming API for a Large Language Model
Large Language Models can power a wide range of applications from chatbots to assistants and intelligent systems.
However, these models can be heavy and slow and you want your users to experience fast yet intelligent systems!

A large language models works by turning your question into tokens and then will keep generating the next token each 
time until it believes the generation should stop.
This means you want to **stream** the output tokens generated by a large language model to the client. 
In this tutorial, we will discuss how to achieve this with Streaming Endpoints in Jina!

## Service Schemas
The first step is to define the streaming service schemas, as you would do in any other serving framework.
The input of the service is the prompt and the maximum number of tokens to generate, while the output is simply the 
token ID:
```python
from docarray import BaseDoc

class PromptDocument(BaseDoc):
    prompt: str
    max_tokens: int

class TokenDocument(BaseDoc):
    token_id: int
```

```{admonition} Note
:class: note

Thanks to DocArray's flexibility, you can actually implement much more flexible services. For instance, you can use 
Tensor types to efficiently stream token logits back to the client and implement complex token sampling strategies on 
the client side.
```

## Service initialization
Our service depends on a large language model. As an example, we will use the `gpt2` model. This is how you would load 
such a model in your executor
```python
from jina import Executor, requests
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

class TokenStreamingExecutor(Executor):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.model = GPT2LMHeadModel.from_pretrained('gpt2')
```

## Implement the streaming endpoint
Our streaming endpoint accepts a `PromptDocument` as input and streams `TokenDocument`s. To stream a document back to 
the client, you can use the `yield` keyword in the endpoint implementation. Therefore, we use the model to generate 
`max_tokens` tokens and yield them until the generation stops: 
```python
    @requests(on='/stream')
    async def task(self, doc: PromptDocument, **kwargs) -> TokenDocument:
        encoded_input = tokenizer(doc.prompt, return_tensors='pt')
        for _ in range(doc.max_tokens):
            output = self.model.generate(**encoded_input, max_new_tokens=1)
            if output[0][-1] == tokenizer.eos_token_id:
                break
            yield TokenDocument(token_id=output[0][-1])
            encoded_input = {'input_ids': output, 'attention_mask': torch.ones(1, len(output[0]))}
```

Learn more about {ref}`streaming endpoints <streaming-endpoints>`.

## Serve and send requests
The final step is to serve the Executor and send requests using the client.
To serve the Executor using gRPC:
```python
from jina import Deployment
with Deployment(uses=TokenStreamingExecutor, port=12345, protocol='grpc', include_gateway=False) as dep:
    dep.block()
```

To send requests from a client:
```python
import asyncio
from jina import Client

async def main():
    client = Client(port=12345, protocol='grpc', asyncio=True)
    tokens = []
    async for doc in client.stream_doc(
        on='/stream', inputs=PromptDocument(prompt='what is the capital of France ?', max_tokens=10), return_type=TokenDocument
    ):
        tokens.append(doc.token_id)
        print(tokenizer.decode(tokens, skip_special_tokens=True))

asyncio.run(main())
```

```text
The capital of France is Paris.
```
