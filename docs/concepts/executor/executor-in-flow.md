(executor-in-flow)=
# Executors in Flows

You can chain Executors together into a {ref}`Flow <flow-cookbook>`, which orchestrates Executors into a processing pipeline to accomplish a task. Documents “flow” through the pipeline and are processed by each Executor in turn.

When developing an Executor to put into a Flow, there are several practices you should follow:

<flow-specific-arguments>
## Flow-specific arguments

When an Executor is instantiated in the context of a Flow, Jina adds a few extra arguments. You can use some of these when developing your Executor's internal logic.

These arguments are `workspace`, `requests`, `metas`, `runtime_args`.

(executor-workspace)=
### `workspace`

Each Executor has a special *workspace* that is reserved for that specific Executor instance.
The `.workspace` property contains the path to this workspace.

This `workspace` is based on the workspace passed when adding the Executor: `flow.add(..., workspace='path/to/workspace/')`.
The final `workspace` is generated by appending `'/<executor_name>/<shard_id>/'`.

This can be provided to the Executor via the Python or {ref}`YAML API <executor-yaml-spec>`.

````{admonition} Hint: Default workspace
:class: hint
If you haven't provided a workspace, the Executor uses a default workspace, defined in `~/.cache/jina/`.
````

(executor-requests)=
### `requests`

By default, an Executor object contains {attr}`~.jina.serve.executors.BaseExecutor.requests` as an attribute when loaded from the Flow. This attribute is a `Dict` describing the mapping between Executor methods and network endpoints: It holds endpoint strings as keys, and pointers to functions as values. 

These can be provided to the Executor via the Python or {ref}`YAML API <executor-yaml-spec>`.

(executor-metas)=
### `metas`

An Executor object contains `metas` as an attribute when loaded from the Flow. It is of [`SimpleNamespace`](https://docs.python.org/3/library/types.html#types.SimpleNamespace) type and contains some key-value information. 

The list of the `metas` are:

- `name`: Name given to the Executor;
- `description`: Description of the Executor (optional, reserved for future-use in auto-docs);

These can be provided to the Executor via Python or {ref}`YAML API <executor-yaml-spec>`.

(executor-runtime-args)
### `runtime_args`

By default, an Executor object contains `runtime_args` as an attribute when loaded from the Flow. It is of [`SimpleNamespace`](https://docs.python.org/3/library/types.html#types.SimpleNamespace) type and contains information in key-value format. 
As the name suggests, `runtime_args` are dynamically determined during runtime, meaning that you don't know the value before running the Executor. These values are often related to the system/network environment around the Executor, and less about the Executor itself, like `shard_id` and `replicas`. They are usually set with the {meth}`~jina.orchestrate.flow.base.Flow.add` method.

The list of the `runtime_args` is:

- `name`: Name given to the Executor. This is dynamically adapted from the `name` in `metas` and depends on some additional arguments like `shard_id`. 
- `replicas`: Number of {ref}`replicas <replicate-executors>` of the same Executor deployed with the Flow.
- `shards`: Number of {ref}`shards <partition-data-by-using-shards>` of the same Executor deployed with the Flow.
- `shard_id`: Identifier of the `shard` corresponding to the given Executor instance.
- `workspace`: Path to be used by the Executor. Note that the actual workspace directory used by the Executor is obtained by appending `'/<executor_name>/<shard_id>/'` to this value.
- `py_modules`: Python package path e.g. `foo.bar.package.module` or file path to the modules needed to import the Executor. This is another way to pass `py-modules` to the Executor from the Flow

You **cannot** provide these through any API. They are generated by the Flow orchestration.

(merge-upstream-documentarrays)=
## Merging upstream DocumentArrays

Often when you're building a Flow, you want an Executor to receive DocumentArrays from multiple upstream Executors. 

```{figure} flow-merge-executor.svg
:width: 70%
:align: center
```

`docs_matrix` and `docs_map` do just that. They Flow-specific arguments that can be used alongside an Executor's {ref}`default arguments <endpoint-arguments>`:

```{code-block} python
---
emphasize-lines: 11, 12
---
from typing import Dict, Union, List, Optional
from jina import Executor, requests, DocumentArray


class MergeExec(Executor):
    @requests
    async def foo(
        self,
        docs: DocumentArray,
        parameters: Dict,
        docs_matrix: Optional[List[DocumentArray]],
        docs_map: Optional[Dict[str, DocumentArray]],
    ) -> Union[DocumentArray, Dict, None]:
        pass
```

- Use `docs_matrix` to receive a List of all incoming DocumentArrays from upstream Executors:

```python
[
    DocumentArray(...),  # from Executor1
    DocumentArray(...),  # from Executor2
    DocumentArray(...),  # from Executor3
]

```

- Use `docs_map` to receive a Dict, where each item's key is the name of an upstream Executor and the value is the DocumentArray coming from that Executor:

```python
{
    'Executor1': DocumentArray(...),
    'Executor2': DocumentArray(...),
    'Executor3': DocumentArray(...),
}
```

(no-reduce)=
### Reducing multiple DocumentArrays to one DocumentArray

The `no_reduce` argument determines whether DocumentArrays are reduced into one when being received:

- To reduce all incoming DocumentArrays into **one single DocumentArray**, do not set `no_reduce` or set it to `False`. The `docs_map` and `docs_matrix` will be `None`.
- To receive **a list all incoming DocumentArrays** set `no_reduce` to `True`. The Executor will receive the DocumentArrays independently under `docs_matrix` and `docs_map`.

```python
from jina import Flow, Executor, requests, Document, DocumentArray


class Exec1(Executor):
    @requests
    def foo(self, docs, **kwargs):
        for doc in docs:
            doc.text = 'Exec1'


class Exec2(Executor):
    @requests
    def foo(self, docs, **kwargs):
        for doc in docs:
            doc.text = 'Exec2'


class MergeExec(Executor):
    @requests
    def foo(self, docs_matrix, **kwargs):
        documents_to_return = DocumentArray()
        for doc1, doc2 in zip(*docs_matrix):
            print(
                f'MergeExec processing pairs of Documents "{doc1.text}" and "{doc2.text}"'
            )
            documents_to_return.append(
                Document(text=f'Document merging from "{doc1.text}" and "{doc2.text}"')
            )
        return documents_to_return


f = (
    Flow()
    .add(uses=Exec1, name='exec1')
    .add(uses=Exec2, name='exec2')
    .add(uses=MergeExec, needs=['exec1', 'exec2'], no_reduce=True)
)

with f:
    returned_docs = f.post(on='/', inputs=Document())

print(f'Resulting documents {returned_docs[0].text}')
```


```shell
────────────────────────── 🎉 Flow is ready to serve! ──────────────────────────
╭────────────── 🔗 Endpoint ───────────────╮
│  ⛓     Protocol                    GRPC  │
│  🏠       Local           0.0.0.0:55761  │
│  🔒     Private     192.168.1.187:55761  │
│  🌍      Public    212.231.186.65:55761  │
╰──────────────────────────────────────────╯

MergeExec processing pairs of Documents "Exec1" and "Exec2"
Resulting documents Document merging from "Exec1" and "Exec2"
```

## Call a Flow from an Executor

To call other another Jina Flow using `Client` from an `Executor`, you also need to use `async def` and async Client.

```python
from jina import Client, Executor, requests, DocumentArray


class DummyExecutor(Executor):
    c = Client(host='grpc://0.0.0.0:51234', asyncio=True)

    @requests
    async def process(self, docs: DocumentArray, **kwargs):
        self.c.post('/', docs)
```

## Serve

Both served and shared Executors can be used as part of a Flow, by adding them as an {ref}`external Executor <external-executors>`.

Having a Gateway may be useful if you want to access your Executor with the {ref}`Client <client>` without an additional Flow. If the Executor is only used inside other Flows, you should define a shared Executor to save the costs of running the Gateway in Kubernetes.
