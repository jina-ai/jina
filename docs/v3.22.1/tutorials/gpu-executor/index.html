<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<meta property="og:title" content="Build a GPU Executor" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://docs.jina.ai/tutorials/gpu-executor/" />
<meta property="og:site_name" content="Jina 3.22.1 Documentation" />
<meta property="og:description" content="This document shows you how to use an Executor on a GPU, both locally and in a Docker container. You will also learn how to use a GPU with pre-built Hub executors. Using a GPU significantly speeds up encoding for most deep learning models, reducing response latency by anything from 5 to 100 times..." />
<meta property="og:image" content="https://docs.jina.ai/_static/banner.png" />
<meta property="og:image:alt" content="Jina 3.22.1 Documentation" />
<meta name="description" content="This document shows you how to use an Executor on a GPU, both locally and in a Docker container. You will also learn how to use a GPU with pre-built Hub executors. Using a GPU significantly speeds up encoding for most deep learning models, reducing response latency by anything from 5 to 100 times..." />
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@JinaAI_">
<meta name="twitter:creator" content="@JinaAI_">
<meta name="description" content="Build multimodal AI services via cloud native technologies 路 Neural Search 路 Generative AI 路 Cloud Native 路 MLOps">
<meta property="og:description" content="Build multimodal AI services via cloud native technologies 路 Neural Search 路 Generative AI 路 Cloud Native 路 MLOps">

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-48ZDWC8GT6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-48ZDWC8GT6');
</script>

<script async defer src="https://buttons.github.io/buttons.js"></script>
    
<link rel="index" title="Index" href="../../genindex/" /><link rel="search" title="Search" href="../../search/" /><link rel="next" title="Build a Streaming API for a Large Language Model" href="../llm-serve/" /><link rel="prev" title="Deploy a model" href="../deploy-model/" />
        <link rel="canonical" href="https://docs.jina.ai/tutorials/gpu-executor.html" />

    <link rel="shortcut icon" href="../../_static/favicon.ico"/><!-- Generated with Sphinx 5.3.0 and Furo 2023.03.27 -->
        <title>Build a GPU Executor - Jina 3.22.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    <link rel="stylesheet" type="text/css" href="../../_static/main.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/docbot.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: #4d4d4d;
  --color-brand-primary: #009191;
  --color-brand-content: #009191;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
    <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
    <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
    <header class="mobile-header">
        <div class="header-left">
            <label class="nav-overlay-icon" for="__navigation">
                <div class="visually-hidden">Toggle site navigation sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-menu"></use>
                    </svg>
                </i>
            </label>
        </div>
        <div class="header-center">
            <a href="../../">
                <div class="brand">Jina 3.22.1 documentation</div>
            </a>
        </div>
        <div class="header-right">
            <div class="theme-toggle-container theme-toggle-header">
                <button class="theme-toggle">
                    <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                    <svg class="theme-icon-when-auto">
                        <use href="#svg-sun-half"></use>
                    </svg>
                    <svg class="theme-icon-when-dark">
                        <use href="#svg-moon"></use>
                    </svg>
                    <svg class="theme-icon-when-light">
                        <use href="#svg-sun"></use>
                    </svg>
                </button>
            </div>
            <label class="toc-overlay-icon toc-header-icon" for="__toc">
                <div class="visually-hidden">Toggle table of contents sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-toc"></use>
                    </svg>
                </i>
            </label>
        </div>
    </header>
    <aside class="sidebar-drawer">
        <div class="sidebar-container">
            
            <div class="sidebar-sticky"><a class="sidebar-brand" href="../../">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../../_static/logo-light.svg" alt="Light Logo" />
    <img class="sidebar-logo only-dark" src="../../_static/logo-dark.svg" alt="Dark Logo" />
  </div>
  
  
</a>
<script>
  function setPrefix(prefix){
    window.location.href = prefix
  }
</script>
<script defer>
  fetch(`https://${window.location.host}/_versions.json`)
  .then((resp) => resp.json())
  .then((data) => {
    var versionSelector = document.getElementsByClassName("version-select")[0];
    var currentPrefix = window.location.href.toString().split(window.location.host)[1].split('/')[1];

    for(var i = 0; i < data.length; i++){
      var option = document.createElement("option");
      option.innerHTML = data[i].version;
      option.value = "/" + data[i].version;
      versionSelector.appendChild(option);
      if(currentPrefix === data[i].version){
        versionSelector.selectedIndex = i + 1;
      }
    }
  })
  .catch((err) => console.log(err));
</script>
<div class="sd-d-flex-row sd-align-major-spaced">
  <a class="github-button" href="https://github.com/jina-ai/jina" data-icon="octicon-star" data-show-count="true" aria-label="Star jina-ai/jina on GitHub" style="opacity: 0;">Star</a>
  <select onChange="setPrefix(this.value)" class="version-select">
    <option value="/">latest</option>
  </select>
</div><form class="sidebar-search-container" method="get" action="../../search/" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
    <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../get-started/install/"><svg aria-hidden="true" class="sd-octicon sd-octicon-desktop-download" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4.927 5.427l2.896 2.896a.25.25 0 00.354 0l2.896-2.896A.25.25 0 0010.896 5H8.75V.75a.75.75 0 10-1.5 0V5H5.104a.25.25 0 00-.177.427z"></path><path d="M1.573 2.573a.25.25 0 00-.073.177v7.5a.25.25 0 00.25.25h12.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-3a.75.75 0 110-1.5h3A1.75 1.75 0 0116 2.75v7.5A1.75 1.75 0 0114.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.75.75 0 0111.25 16h-6.5a.75.75 0 01-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 010 10.25v-7.5A1.75 1.75 0 011.75 1h3a.75.75 0 010 1.5h-3a.25.25 0 00-.177.073zM6.982 12a5.72 5.72 0 01-.765 2.5h3.566a5.72 5.72 0 01-.765-2.5H6.982z"></path></svg> Install</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../get-started/install/docker/">Via Docker Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../get-started/install/apple-silicon-m1-m2/">On Apple Silicon</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../get-started/install/windows/">On Windows</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../get-started/install/troubleshooting/">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../get-started/create-app/"><span class="fas fa-folder-plus"></span> Create First Project</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Concepts</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../concepts/preliminaries/"><span class="fas fa-egg"></span> Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/preliminaries/coding-in-python-yaml/">Coding in Python/YAML</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../concepts/serving/"><span class="fas fa-gears"></span> Serving</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../concepts/serving/executor/">Executor</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/create/">Create</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/add-endpoints/">Add Endpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/serve/">Serve</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/dynamic-batching/">Dynamic Batching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/health-check/">Health Check</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/hot-reload/">Hot Reload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/file-structure/">File Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/containerize/">Containerize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/instrumentation/">Instrumentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/yaml-spec/"><svg aria-hidden="true" class="sd-octicon sd-octicon-file-code" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z" fill-rule="evenodd"></path></svg> YAML specification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../concepts/serving/gateway/">Gateway</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/gateway/health-check/">Health Check</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/gateway/rate-limit/">Rate Limit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/gateway/customize-http-endpoints/">Customize HTTP endpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/gateway/customization/">Customization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/gateway/yaml-spec/"><svg aria-hidden="true" class="sd-octicon sd-octicon-file-code" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z" fill-rule="evenodd"></path></svg> YAML specification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../concepts/orchestration/"><span class="fas fa-network-wired"></span> Orchestration</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/deployment/">Deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/flow/">Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/add-executors/">Add Executors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/scale-out/">Scale Out</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/hot-reload/">Hot Reload</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/handle-exceptions/">Handle Exceptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/readiness/">Readiness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/health-check/">Health Check</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/instrumentation/">Instrumentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/troubleshooting-on-multiprocess/">Troubleshooting on Multiprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/orchestration/yaml-spec/"><svg aria-hidden="true" class="sd-octicon sd-octicon-file-code" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z" fill-rule="evenodd"></path></svg> YAML specification</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../concepts/client/"><span class="fas fa-laptop-code"></span> Client</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/client/send-receive-data/">Send &amp; Receive Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/client/send-parameters/">Send Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/client/send-graphql-mutation/">Send GraphQL Mutation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/client/transient-errors/">Transient Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/client/callbacks/">Callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/client/rate-limit/">Rate Limit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/client/instrumentation/">Instrumentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/client/third-party-clients/">Third-party clients</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud Native</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../cloud-nativeness/k8s/"><span class="fas fa-dharmachakra"></span> Kubernetes Support</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../cloud-nativeness/kubernetes/">Deploy on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cloud-nativeness/docker-compose/"><span class="fab fa-docker"></span> Docker Compose Support</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../cloud-nativeness/opentelemetry/"><svg aria-hidden="true" class="sd-octicon sd-octicon-telescope-fill" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M8.531 10.21a.75.75 0 01.944.253l2.644 3.864a.75.75 0 11-1.238.847L9 12.424v2.826a.75.75 0 01-1.5 0v-2.826l-1.881 2.75a.75.75 0 01-1.238-.848l2.048-2.992a.75.75 0 01.293-.252l1.81-.871zM11.905.42a1.5 1.5 0 012.144.49l1.692 2.93a1.5 1.5 0 01-.649 2.102L2.895 11.815a1.5 1.5 0 01-1.95-.602l-.68-1.176a1.5 1.5 0 01.455-1.99L11.905.422zM3.279 8.119l.835 1.445 1.355-.653-.947-1.64-1.243.848zm7.728-1.874L9.6 3.808l1.243-.848 1.52 2.631-1.356.653z" fill-rule="evenodd"></path></svg> OpenTelemetry Support</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../cloud-nativeness/opentelemetry-migration/">Migrate from Prometheus/Grafana to OpenTelemetry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cloud-nativeness/monitoring/">Prometheus/Grafana Support (Legacy)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../jina-ai-cloud/"><svg aria-hidden="true" class="sd-octicon sd-octicon-beaker" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M5 5.782V2.5h-.25a.75.75 0 010-1.5h6.5a.75.75 0 010 1.5H11v3.282l3.666 5.76C15.619 13.04 14.543 15 12.767 15H3.233c-1.776 0-2.852-1.96-1.899-3.458L5 5.782zM9.5 2.5h-3V6a.75.75 0 01-.117.403L4.73 9h6.54L9.617 6.403A.75.75 0 019.5 6V2.5zm-6.9 9.847L3.775 10.5h8.45l1.175 1.847a.75.75 0 01-.633 1.153H3.233a.75.75 0 01-.633-1.153z" fill-rule="evenodd"></path></svg> Jina AI Cloud</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../jina-ai-cloud/login/">Login &amp; Token Management</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../concepts/serving/executor/hub/">Executor Hub</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/hub/hub-portal/">Portal</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/hub/create-hub-executor/">Create</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/hub/push-executor/">Publish</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/hub/use-hub-executor/">Use</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/serving/executor/hub/debug-executor/">Debug</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../concepts/jcloud/">Jina AI Cloud Hosting</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/jcloud/configuration/"><svg aria-hidden="true" class="sd-octicon sd-octicon-file-code" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z" fill-rule="evenodd"></path></svg> Configuration</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api-rst/"><span class="fab fa-python"></span> Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli/"><svg aria-hidden="true" class="sd-octicon sd-octicon-terminal" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M0 2.75C0 1.784.784 1 1.75 1h12.5c.966 0 1.75.784 1.75 1.75v10.5A1.75 1.75 0 0114.25 15H1.75A1.75 1.75 0 010 13.25V2.75zm1.75-.25a.25.25 0 00-.25.25v10.5c0 .138.112.25.25.25h12.5a.25.25 0 00.25-.25V2.75a.25.25 0 00-.25-.25H1.75zM7.25 8a.75.75 0 01-.22.53l-2.25 2.25a.75.75 0 11-1.06-1.06L5.44 8 3.72 6.28a.75.75 0 111.06-1.06l2.25 2.25c.141.14.22.331.22.53zm1.5 1.5a.75.75 0 000 1.5h3a.75.75 0 000-1.5h-3z" fill-rule="evenodd"></path></svg> Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../yaml-spec/"><svg aria-hidden="true" class="sd-octicon sd-octicon-file-code" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z" fill-rule="evenodd"></path></svg> YAML Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../envs/"><svg aria-hidden="true" class="sd-octicon sd-octicon-list-unordered" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M2 4a1 1 0 100-2 1 1 0 000 2zm3.75-1.5a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5zm0 5a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5zm0 5a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5zM3 8a1 1 0 11-2 0 1 1 0 012 0zm-1 6a1 1 0 100-2 1 1 0 000 2z" fill-rule="evenodd"></path></svg> Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../telemetry/"><span class="fas fa-tower-cell"></span> Telemetry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../proto/docs/">Protocol Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docarray-support/">DocArray support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../deploy-model/">Deploy a model</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Build a GPU Executor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm-serve/">Build a Streaming API for a Large Language Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Legacy Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://docs2.jina.ai/">Jina 2 Documentation</a></li>
</ul>

    <p class="caption" role="heading"><span class="caption-text">Ecosystem</span></p>
    <ul>
        <li class="toctree-l1">
            <a class="reference internal" href="#">
                <img class="sidebar-ecosys-logo only-light-line" src="../../_static/search-light.svg">
                <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/search-dark.svg">
                Jina</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://cloud.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/hub-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/hub-dark.svg">
            Jina Hub</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://finetuner.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/finetuner-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/finetuner-dark.svg">
            Finetuner</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://docs.docarray.org">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/docarray-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/docarray-dark.svg">
            DocArray</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://clip-as-service.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/cas-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/cas-dark.svg">
            CLIP-as-service</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://github.com/jina-ai/jcloud">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/JCloud-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/JCloud-dark.svg">
            JCloud</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://now.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/now-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/now-dark.svg">
            NOW</a></li>
    </ul>
</div>

</div>

            </div>
            
        </div>
    </aside>
    <div class="main">
        <div class="content">
            <div class="article-container">
                <a href="#" class="back-to-top muted-link">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
                    </svg>
                    <span>Back to top</span>
                </a>
                <div class="content-icon-container"><div class="theme-toggle-container theme-toggle-content">
                        <button class="theme-toggle">
                            <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                            <svg class="theme-icon-when-auto">
                                <use href="#svg-sun-half"></use>
                            </svg>
                            <svg class="theme-icon-when-dark">
                                <use href="#svg-moon"></use>
                            </svg>
                            <svg class="theme-icon-when-light">
                                <use href="#svg-sun"></use>
                            </svg>
                        </button>
                    </div>
                    <label class="toc-overlay-icon toc-content-icon"
                           for="__toc">
                        <div class="visually-hidden">Toggle table of contents sidebar</div>
                        <i class="icon">
                            <svg>
                                <use href="#svg-toc"></use>
                            </svg>
                        </i>
                    </label>
                </div>
                <article role="main">
                    <section id="build-a-gpu-executor">
<span id="gpu-executor"></span><h1>Build a GPU Executor<a class="headerlink" href="#build-a-gpu-executor" title="Permalink to this heading">#</a></h1>
<p>This document shows you how to use an <code class="xref py py-class docutils literal notranslate"><span class="pre">Executor</span></code> on a GPU, both locally and in a
Docker container. You will also learn how to use a GPU with pre-built Hub executors.</p>
<p>Using a GPU significantly speeds up encoding for most deep learning models,
reducing response latency by anything from 5 to 100 times, depending on the model and inputs used.</p>
<div class="caution admonition">
<p class="admonition-title">Important</p>
<p>This tutorial assumes familiarity with basic Jina concepts, such as Document, <a class="reference internal" href="#../concepts/executor/index"><span class="xref myst">Executor</span></a>, and <a class="reference internal" href="#../concepts/deployment/index"><span class="xref myst">Deployment</span></a>. Some knowledge of <a class="reference internal" href="#../concepts/executor/hub/index"><span class="xref myst">Executor Hub</span></a> is also needed for the last part of the tutorial.</p>
</div>
<section id="jina-and-gpus-in-a-nutshell">
<h2>Jina and GPUs in a nutshell<a class="headerlink" href="#jina-and-gpus-in-a-nutshell" title="Permalink to this heading">#</a></h2>
<p>For a thorough walkthrough of using GPU resources in your code, check the full tutorial in the <a class="reference internal" href="#gpu-prerequisites"><span class="std std-ref">next section</span></a>.</p>
<p>If you already know how to use your GPU, just proceed like you usually would in your machine learning framework of choice.
Jina lets you use GPUs like you would in a Python script or Docker
container, without imposing additional requirements or configuration.</p>
<p>Heres a minimal working example, written in PyTorch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">docarray</span> <span class="kn">import</span> <span class="n">DocList</span><span class="p">,</span> <span class="n">BaseDoc</span>
<span class="kn">from</span> <span class="nn">docarray.typing</span> <span class="kn">import</span> <span class="n">AnyTensor</span>
<span class="kn">from</span> <span class="nn">jina</span> <span class="kn">import</span> <span class="n">Executor</span><span class="p">,</span> <span class="n">requests</span>


<span class="k">class</span> <span class="nc">MyDoc</span><span class="p">(</span><span class="n">BaseDoc</span><span class="p">):</span>
    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="n">embedding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AnyTensor</span><span class="p">[</span><span class="mi">5</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">class</span> <span class="nc">MyGPUExec</span><span class="p">(</span><span class="n">Executor</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

    <span class="nd">@requests</span>
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">:</span> <span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">]:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
            <span class="c1"># Generate random embeddings</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">docs</span><span class="p">),</span> <span class="mi">5</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">docs</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embeddings</span>
            <span class="n">embedding_device</span> <span class="o">=</span> <span class="s1">&#39;GPU&#39;</span> <span class="k">if</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="s1">&#39;CPU&#39;</span>
            <span class="n">docs</span><span class="o">.</span><span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Embeddings calculated on </span><span class="si">{</span><span class="n">embedding_device</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--1">Use with CPU</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">docarray</span> <span class="kn">import</span> <span class="n">DocList</span><span class="p">,</span> <span class="n">BaseDoc</span>
<span class="kn">from</span> <span class="nn">docarray.typing</span> <span class="kn">import</span> <span class="n">AnyTensor</span>
<span class="kn">from</span> <span class="nn">jina</span> <span class="kn">import</span> <span class="n">Deployment</span>

<span class="n">dep</span> <span class="o">=</span> <span class="n">Deployment</span><span class="p">(</span><span class="n">uses</span><span class="o">=</span><span class="n">MyGPUExec</span><span class="p">,</span> <span class="n">uses_with</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cpu&#39;</span><span class="p">})</span>
<span class="n">docs</span> <span class="o">=</span>  <span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">]([</span><span class="n">MyDoc</span><span class="p">()])</span>

<span class="k">with</span> <span class="n">dep</span><span class="p">:</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">dep</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">on</span><span class="o">=</span><span class="s1">&#39;/encode&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">docs</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Document embedding: </span><span class="si">{</span><span class="n">docs</span><span class="o">.</span><span class="n">embedding</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">docs</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">           </span>Deployment@80<span class="o">[</span>I<span class="o">]</span>:<span class="w"> </span>Deployment<span class="w"> </span>is<span class="w"> </span>ready<span class="w"> </span>to<span class="w"> </span>use!
<span class="w">	</span><span class="w"> </span>Protocol:<span class="w"> 		</span>GRPC
<span class="w">	</span><span class="w"> </span>Local<span class="w"> </span>access:<span class="w">	</span><span class="m">0</span>.0.0.0:49618
<span class="w">	</span><span class="w"> </span>Private<span class="w"> </span>network:<span class="w">	</span><span class="m">172</span>.28.0.2:49618
<span class="w">	</span><span class="w"> </span>Public<span class="w"> </span>address:<span class="w">	</span><span class="m">34</span>.67.105.220:49618
Document<span class="w"> </span>embedding:<span class="w"> </span>tensor<span class="o">([[</span><span class="m">0</span>.1769,<span class="w"> </span><span class="m">0</span>.1557,<span class="w"> </span><span class="m">0</span>.9266,<span class="w"> </span><span class="m">0</span>.8655,<span class="w"> </span><span class="m">0</span>.6291<span class="o">]])</span>
<span class="o">[</span><span class="s1">&#39;Embeddings calculated on CPU&#39;</span><span class="o">]</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--2">Use with GPU</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">docarray</span> <span class="kn">import</span> <span class="n">DocList</span><span class="p">,</span> <span class="n">BaseDoc</span>
<span class="kn">from</span> <span class="nn">docarray.typing</span> <span class="kn">import</span> <span class="n">AnyTensor</span>
<span class="kn">from</span> <span class="nn">jina</span> <span class="kn">import</span> <span class="n">Deployment</span>

<span class="n">dep</span> <span class="o">=</span> <span class="n">Deployment</span><span class="p">(</span><span class="n">uses</span><span class="o">=</span><span class="n">MyGPUExec</span><span class="p">,</span> <span class="n">uses_with</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">})</span>
<span class="n">docs</span> <span class="o">=</span>  <span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">]([</span><span class="n">MyDoc</span><span class="p">()])</span>

<span class="k">with</span> <span class="n">dep</span><span class="p">:</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">dep</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">on</span><span class="o">=</span><span class="s1">&#39;/encode&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">docs</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Document embedding: </span><span class="si">{</span><span class="n">docs</span><span class="o">.</span><span class="n">embedding</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">docs</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">           </span>Deployment@80<span class="o">[</span>I<span class="o">]</span>:<span class="w"> </span>Deployment<span class="w"> </span>is<span class="w"> </span>ready<span class="w"> </span>to<span class="w"> </span>use!
<span class="w">	</span><span class="w"> </span>Protocol:<span class="w"> 		</span>GRPC
<span class="w">	</span><span class="w"> </span>Local<span class="w"> </span>access:<span class="w">	</span><span class="m">0</span>.0.0.0:56276
<span class="w">	</span><span class="w"> </span>Private<span class="w"> </span>network:<span class="w">	</span><span class="m">172</span>.28.0.2:56276
<span class="w">	</span><span class="w"> </span>Public<span class="w"> </span>address:<span class="w">	</span><span class="m">34</span>.67.105.220:56276
Document<span class="w"> </span>embedding:<span class="w"> </span>tensor<span class="o">([[</span><span class="m">0</span>.6888,<span class="w"> </span><span class="m">0</span>.8646,<span class="w"> </span><span class="m">0</span>.0422,<span class="w"> </span><span class="m">0</span>.8501,<span class="w"> </span><span class="m">0</span>.4016<span class="o">]])</span>
<span class="o">[</span><span class="s1">&#39;Embeddings calculated on GPU&#39;</span><span class="o">]</span>
</pre></div>
</div>
</div>
</div>
<p>Just like that, your code runs on GPU, inside a Deployment.</p>
<p>Next, we will go through a more fleshed out example in detail, where we use a language model to embed text in our
Documents - all on GPU, and thus blazingly fast.</p>
</section>
<section id="prerequisites">
<span id="gpu-prerequisites"></span><h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">#</a></h2>
<p>For this tutorial, you need to work on a machine with an NVIDIA graphics card. If you
dont have such a machine at home, you can use various free cloud platforms (like Google Colab or Kaggle kernels).</p>
<p>Also ensure you have a recent version of <a class="reference external" href="https://www.nvidia.com/Download/index.aspx">NVIDIA drivers</a>
installed. You dont need to install CUDA for this tutorial, but note that depending on
the deep learning framework that you use, that might be required (for local execution).</p>
<p>For the Docker part of the tutorial you will also need to have <a class="reference external" href="https://docs.docker.com/get-docker/">Docker</a> and
<a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker</a> installed.</p>
<p>To run Python scripts you need a virtual environment (for example <a class="reference external" href="https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environment">venv</a> or <a class="reference external" href="https://conda.io/projects/conda/en/latest/user-guide/getting-started.html#managing-environments">conda</a>), and install Jina inside it using:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>jina
</pre></div>
</div>
</section>
<section id="setting-up-the-executor">
<h2>Setting up the Executor<a class="headerlink" href="#setting-up-the-executor" title="Permalink to this heading">#</a></h2>
<div class="hint admonition">
<p class="admonition-title">Executor Hub</p>
<p>Lets create an Executor using <code class="docutils literal notranslate"><span class="pre">jina</span> <span class="pre">hub</span> <span class="pre">new</span></code>. This creates your Executor locally
and privately, and makes it quick and easy to run your
Executor inside a Docker container, or (if you so choose) to publish it to Executor Hub later.</p>
</div>
<p>Well create a simple sentence encoder, and start by creating the Executor
skeleton using Jinas CLI:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>jina<span class="w"> </span>hub<span class="w"> </span>new
</pre></div>
</div>
<p>When prompted, name your Executor <code class="docutils literal notranslate"><span class="pre">SentenceEncoder</span></code>, and accept the default
folder - this creates a <code class="docutils literal notranslate"><span class="pre">SentenceEncoder/</span></code> folder inside your current
directory, which will be our working directory for this tutorial.</p>
<p>For many questions you can accept the default options. However:</p>
<ul class="simple">
<li><p>Select <code class="docutils literal notranslate"><span class="pre">y</span></code> when prompted for advanced configuration.</p></li>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">y</span></code> when prompted to create a <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code>.</p></li>
</ul>
<p>In the end, you should be greeted with suggested next steps.</p>
<details>
  <summary> Next steps </summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="w"> </span>Next<span class="w"> </span>steps<span class="w"> </span>
<span class="w">                                                                                            </span>
<span class="w">  </span>Congrats!<span class="w"> </span>You<span class="w"> </span>have<span class="w"> </span>successfully<span class="w"> </span>created<span class="w"> </span>an<span class="w"> </span>Executor!<span class="w"> </span>Here<span class="w"> </span>are<span class="w"> </span>the<span class="w"> </span>next<span class="w"> </span>steps:<span class="w">             </span>
<span class="w">  </span><span class="w"> </span><span class="m">1</span>.<span class="w"> </span>Check<span class="w"> </span>out<span class="w"> </span>the<span class="w"> </span>generated<span class="w"> </span>Executor<span class="w"> </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span><span class="m">1</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>/home/ubuntu/SentenceEncoder<span class="w">                                                  </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span><span class="m">2</span><span class="w"> </span>ls<span class="w">                                                                               </span><span class="w">  </span>
<span class="w">  </span>扳<span class="w">  </span>
<span class="w">  </span><span class="w"> </span><span class="m">2</span>.<span class="w"> </span>Understand<span class="w"> </span>folder<span class="w"> </span>structure<span class="w"> </span><span class="w">  </span>
<span class="w">  </span><span class="w">                                                                                      </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span>Filena<span class="w">   </span>Description<span class="w">                                                              </span><span class="w">  </span>
<span class="w">  </span><span class="w">  </span><span class="w">  </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span>config<span class="w">   </span>The<span class="w"> </span>YAML<span class="w"> </span>config<span class="w"> </span>file<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>Executor.<span class="w"> </span>You<span class="w"> </span>can<span class="w"> </span>define<span class="w"> </span>__init__<span class="w"> </span>argumen<span class="w">   </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w"> </span>config.yml<span class="w"> </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w">   </span><span class="m">1</span><span class="w">                                            </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w">   </span><span class="m">2</span><span class="w"> </span>jtype:<span class="w"> </span>SentenceEncoder<span class="w">                     </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w">   </span><span class="m">3</span><span class="w"> </span>with:<span class="w">                                      </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w">   </span><span class="m">4</span><span class="w">     </span>foo:<span class="w"> </span><span class="m">1</span><span class="w">                                 </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w">   </span><span class="m">5</span><span class="w">     </span>bar:<span class="w"> </span>hello<span class="w">                             </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w">   </span><span class="m">6</span><span class="w"> </span>metas:<span class="w">                                     </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w">   </span><span class="m">7</span><span class="w">     </span>py_modules:<span class="w">                            </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w">   </span><span class="m">8</span><span class="w">         </span>-<span class="w"> </span>executor.py<span class="w">                      </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span><span class="w">   </span><span class="m">9</span><span class="w">                                            </span><span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">             </span>扳<span class="w">                       </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span>Docker<span class="w">   </span>The<span class="w"> </span>Dockerfile<span class="w"> </span>describes<span class="w"> </span>how<span class="w"> </span>this<span class="w"> </span>executor<span class="w"> </span>will<span class="w"> </span>be<span class="w"> </span>built.<span class="w">                </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span>execut<span class="w">   </span>The<span class="w"> </span>main<span class="w"> </span>logic<span class="w"> </span>file<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>Executor.<span class="w">                                     </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span>manife<span class="w">   </span>Metadata<span class="w"> </span><span class="k">for</span><span class="w"> </span>the<span class="w"> </span>Executor,<span class="w"> </span><span class="k">for</span><span class="w"> </span>better<span class="w"> </span>appeal<span class="w"> </span>on<span class="w"> </span>Executor<span class="w"> </span>Hub.<span class="w">                </span><span class="w">  </span>
<span class="w">  </span><span class="w">                                                                                      </span><span class="w">  </span>
<span class="w">  </span><span class="w">               </span>Field<span class="w">   </span>Description<span class="w">                                                    </span><span class="w">  </span>
<span class="w">  </span><span class="w">              </span><span class="w">    </span><span class="w">  </span>
<span class="w">  </span><span class="w">               </span>name<span class="w">    </span>Human-readable<span class="w"> </span>title<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>Executor<span class="w">                           </span><span class="w">  </span>
<span class="w">  </span><span class="w">               </span>desc<span class="w">   </span>Human-readable<span class="w"> </span>description<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>Executor<span class="w">                     </span><span class="w">  </span>
<span class="w">  </span><span class="w">               </span>url<span class="w">     </span>URL<span class="w"> </span>to<span class="w"> </span>find<span class="w"> </span>more<span class="w"> </span>information<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>Executor<span class="w"> </span><span class="o">(</span>e.g.<span class="w"> </span>GitHub<span class="w">     </span><span class="w">  </span>
<span class="w">  </span><span class="w">               </span>keyw<span class="w">   </span>Keywords<span class="w"> </span>that<span class="w"> </span><span class="nb">help</span><span class="w"> </span>user<span class="w"> </span>find<span class="w"> </span>the<span class="w"> </span>Executor<span class="w">                      </span><span class="w">  </span>
<span class="w">  </span><span class="w">                                                                                      </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span>README<span class="w">   </span>A<span class="w"> </span>usage<span class="w"> </span>guide<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>Executor.<span class="w">                                           </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span>requir<span class="w">   </span>The<span class="w"> </span>Python<span class="w"> </span>dependencies<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>Executor.<span class="w">                                 </span><span class="w">  </span>
<span class="w">  </span><span class="w">                                                                                      </span><span class="w">  </span>
<span class="w">  </span>扳<span class="w">  </span>
<span class="w">  </span><span class="w"> </span><span class="m">3</span>.<span class="w"> </span>Share<span class="w"> </span>it<span class="w"> </span>to<span class="w"> </span>Executor<span class="w"> </span>Hub<span class="w"> </span><span class="w">  </span>
<span class="w">  </span><span class="w">   </span><span class="m">1</span><span class="w"> </span>jina<span class="w"> </span>hub<span class="w"> </span>push<span class="w"> </span>/home/ubuntu/SentenceEncoder<span class="w">                                       </span><span class="w">  </span>
<span class="w">  </span>扳<span class="w">  </span>
扳
</pre></div>
</div>
</details>
<p>Now lets move to the newly created Executor directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>SentenceEncoder
</pre></div>
</div>
<p>Continue by specifying our requirements in <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>sentence-transformers==2.0.0
</pre></div>
</div>
<p>And installing them using:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
<div class="hint admonition">
<p class="admonition-title">Do I need to install CUDA?</p>
<p>All machine learning frameworks rely on CUDA for running on a GPU. However, whether you
need CUDA installed on your system or not depends on the framework that you use.</p>
<p>In this tutorial, we use PyTorch, which already includes the necessary
CUDA binaries in its distribution. However, other frameworks, such as TensorFlow, require
you to install CUDA yourself.</p>
</div>
<div class="hint admonition">
<p class="admonition-title">Install only what you need</p>
<p>In this example we install the GPU-enabled version of PyTorch, which is the default
version when installing from PyPI. However, if you know that you only need to use your
Executor on CPU, you can save a lot of space (hundreds of MBs, or even GBs) by installing
CPU-only versions of your requirements. This translates into faster startup times
when using Docker containers.</p>
<p>In our case, we could change the <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file to install a CPU-only version
of PyTorch:</p>
<div class="text docutils">
<p>-f https://download.pytorch.org/whl/torch_stable.html
sentence-transformers
torch==1.9.0+cpu</p>
</div>
</div>
<p>Now lets fill the <code class="docutils literal notranslate"><span class="pre">executor.py</span></code> file with the actual Executor code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">docarray</span> <span class="kn">import</span> <span class="n">DocList</span><span class="p">,</span> <span class="n">BaseDoc</span>
<span class="kn">from</span> <span class="nn">docarray.typing</span> <span class="kn">import</span> <span class="n">AnyTensor</span>
<span class="kn">from</span> <span class="nn">jina</span> <span class="kn">import</span> <span class="n">Executor</span><span class="p">,</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="k">class</span> <span class="nc">MyDoc</span><span class="p">(</span><span class="n">BaseDoc</span><span class="p">):</span>
    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="n">embedding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AnyTensor</span><span class="p">[</span><span class="mi">5</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">class</span> <span class="nc">SentenceEncoder</span><span class="p">(</span><span class="n">Executor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A simple sentence encoder that can be run on a CPU or a GPU</span>

<span class="sd">    :param device: The pytorch device that the model is on, e.g. &#39;cpu&#39;, &#39;cuda&#39;, &#39;cuda:1&#39;</span>
<span class="hll"><span class="sd">    &quot;&quot;&quot;</span>
</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># Move the model to device</span>

    <span class="nd">@requests</span>
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">:</span> <span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add text-based embeddings to all documents&quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="o">.</span><span class="n">texts</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
        <span class="n">docs</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span>
</pre></div>
</div>
<p>Here all the device-specific magic happens on the two highlighted lines - when we create the
<code class="docutils literal notranslate"><span class="pre">SentenceEncoder</span></code> class instance we pass it the device, and then we move the PyTorch
model to our device. These are also the exact same steps to use in a standalone Python script.</p>
<p>To see how we would pass the device we want the Executor to use,
lets create another file - <code class="docutils literal notranslate"><span class="pre">main.py</span></code>, to demonstrate the usage of this
encoder by encoding 10,000 text documents.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">jina</span> <span class="kn">import</span> <span class="n">Deployment</span>
<span class="kn">from</span> <span class="nn">docarray</span> <span class="kn">import</span> <span class="n">DocList</span><span class="p">,</span> <span class="n">BaseDoc</span>
<span class="kn">from</span> <span class="nn">docarray.typing</span> <span class="kn">import</span> <span class="n">AnyTensor</span>
<span class="kn">from</span> <span class="nn">executor</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>


<span class="k">class</span> <span class="nc">MyDoc</span><span class="p">(</span><span class="n">BaseDoc</span><span class="p">):</span>
    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="n">embedding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AnyTensor</span><span class="p">[</span><span class="mi">5</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">generate_docs</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10_000</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">MyDoc</span><span class="p">(</span>
            <span class="n">text</span><span class="o">=</span><span class="s1">&#39;Using a GPU allows you to significantly speed up encoding.&#39;</span>
        <span class="p">)</span>


<span class="n">dep</span> <span class="o">=</span> <span class="n">Deployment</span><span class="p">(</span><span class="n">uses</span><span class="o">=</span><span class="n">SentenceEncoder</span><span class="p">,</span> <span class="n">uses_with</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cpu&#39;</span><span class="p">})</span>

<span class="k">with</span> <span class="n">dep</span><span class="p">:</span>
    <span class="n">dep</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">on</span><span class="o">=</span><span class="s1">&#39;/encode&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">generate_docs</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">request_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="running-on-gpu-and-cpu-locally">
<h2>Running on GPU and CPU locally<a class="headerlink" href="#running-on-gpu-and-cpu-locally" title="Permalink to this heading">#</a></h2>
<p>We can observe the speed up by running the same code on both the CPU and GPU.</p>
<p>To toggle between the two, set your device type to <code class="docutils literal notranslate"><span class="pre">'cuda'</span></code>, and your GPU will take over the work:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="gi">+ dep = Deployment(uses=SentenceEncoder, uses_with={&#39;device&#39;: &#39;cuda&#39;})</span>
<span class="gd">- dep = Deployment(uses=SentenceEncoder, uses_with={&#39;device&#39;: &#39;cpu&#39;})</span>
</pre></div>
</div>
<p>Then, run the script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main.py
</pre></div>
</div>
<p>And compare the results:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--1-input--1" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--1">CPU</label><div class="tab-content docutils">
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">      </span>executor0@26554<span class="o">[</span>L<span class="o">]</span>:ready<span class="w"> </span>and<span class="w"> </span>listening
<span class="w">        </span>gateway@26554<span class="o">[</span>L<span class="o">]</span>:ready<span class="w"> </span>and<span class="w"> </span>listening
<span class="w">           </span>Deployment@26554<span class="o">[</span>I<span class="o">]</span>:<span class="w"> </span>Deployment<span class="w"> </span>is<span class="w"> </span>ready<span class="w"> </span>to<span class="w"> </span>use!
<span class="w">        </span><span class="w"> </span>Protocol:<span class="w">            </span>GRPC
<span class="w">        </span><span class="w"> </span>Local<span class="w"> </span>access:<span class="w">        </span><span class="m">0</span>.0.0.0:56969
<span class="w">        </span><span class="w"> </span>Private<span class="w"> </span>network:<span class="w">     </span><span class="m">172</span>.31.39.70:56969
<span class="w">        </span><span class="w"> </span>Public<span class="w"> </span>address:<span class="w">      </span><span class="m">52</span>.59.231.246:56969
Working...<span class="w"> </span>糕<span class="w"> </span><span class="m">0</span>:00:20<span class="w"> </span><span class="m">15</span>.1<span class="w"> </span>step/s<span class="w"> </span><span class="m">314</span><span class="w"> </span>steps<span class="w"> </span><span class="k">done</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">20</span><span class="w"> </span>seconds
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--1-input--2" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--2">GPU</label><div class="tab-content docutils">
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">      </span>executor0@21032<span class="o">[</span>L<span class="o">]</span>:ready<span class="w"> </span>and<span class="w"> </span>listening
<span class="w">        </span>gateway@21032<span class="o">[</span>L<span class="o">]</span>:ready<span class="w"> </span>and<span class="w"> </span>listening
<span class="w">           </span>Deployment@21032<span class="o">[</span>I<span class="o">]</span>:<span class="w"> </span>Deployment<span class="w"> </span>is<span class="w"> </span>ready<span class="w"> </span>to<span class="w"> </span>use!
<span class="w">        </span><span class="w"> </span>Protocol:<span class="w">            </span>GRPC
<span class="w">        </span><span class="w"> </span>Local<span class="w"> </span>access:<span class="w">        </span><span class="m">0</span>.0.0.0:54255
<span class="w">        </span><span class="w"> </span>Private<span class="w"> </span>network:<span class="w">     </span><span class="m">172</span>.31.39.70:54255
<span class="w">        </span><span class="w"> </span>Public<span class="w"> </span>address:<span class="w">      </span><span class="m">52</span>.59.231.246:54255
Working...<span class="w"> </span>糕<span class="w"> </span><span class="m">0</span>:00:03<span class="w"> </span><span class="m">90</span>.9<span class="w"> </span>step/s<span class="w"> </span><span class="m">314</span><span class="w"> </span>steps<span class="w"> </span><span class="k">done</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">3</span><span class="w"> </span>seconds
</pre></div>
</div>
</div>
</div>
<p>Running this code on a <code class="docutils literal notranslate"><span class="pre">g4dn.xlarge</span></code> AWS instance with a single NVIDIA T4 GPU attached, we can see that embedding
time decreases from 20s to 3s by running on GPU.
Thats more than a <strong>6x speedup!</strong> And thats not even the best we can do - if we increase the batch size to max out the GPUs memory we would get even larger speedups. But such optimizations are beyond the scope of this tutorial.</p>
<div class="hint admonition">
<p class="admonition-title">Note</p>
<p>Youve probably noticed that there was a delay (about 3 seconds) when creating the Deployment.
This is because the weights of our model had to be transfered from CPU to GPU when we
initialized the Executor. However, this action only occurs once in the lifetime of the Executor,
so for most use cases we dont need to worry about it.</p>
</div>
</section>
<section id="using-gpu-in-a-container">
<h2>Using GPU in a container<a class="headerlink" href="#using-gpu-in-a-container" title="Permalink to this heading">#</a></h2>
<div class="caution admonition">
<p class="admonition-title">Using your GPU inside a container</p>
<p>For this part of the tutorial, you need to <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">install <code class="docutils literal notranslate"><span class="pre">nvidia-container-toolkit</span></code></a>.</p>
</div>
<p>When you use your Executor in production you most likely want it in a Docker container, to provide proper environment isolation and easily use it on any device.</p>
<p>Using GPU-enabled Executors in this case is no harder than using them locally. We dont even need to modify the default <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code>.</p>
<div class="hint admonition">
<p class="admonition-title">Choosing the right base image</p>
<p>In our case we use the default <code class="docutils literal notranslate"><span class="pre">jinaai/jina:latest</span></code> base image. However, parallel to the comments about installing CUDA locally, you may need a different base image depending on your framework.</p>
<p>If you need CUDA installed in the image, you usually have two options: either take <code class="docutils literal notranslate"><span class="pre">nvidia/cuda</span></code> for the base image, or take the official GPU-enabled image of your framework, for example, <code class="docutils literal notranslate"><span class="pre">tensorflow/tensorflow:2.6.0-gpu</span></code>.</p>
</div>
<p>The other file we care about in this case is <code class="docutils literal notranslate"><span class="pre">config.yml</span></code>, and here the default version works as well. Lets build the Docker image:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>sentence-encoder<span class="w"> </span>.
</pre></div>
</div>
<p>You can run the container to check that everything is working well:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>sentence-encoder
</pre></div>
</div>
<p>Lets use the Docker version of our encoder with the GPU. If youve dealt with GPUs in containers before, you may remember that to use a GPU inside the container you need to pass <code class="docutils literal notranslate"><span class="pre">--gpus</span> <span class="pre">all</span></code> option to the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command. Jina lets you do just that.</p>
<p>We need to modify our <code class="docutils literal notranslate"><span class="pre">main.py</span></code> script to use a GPU-base containerized Executor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">jina</span> <span class="kn">import</span> <span class="n">Deployment</span>
<span class="kn">from</span> <span class="nn">docarray</span> <span class="kn">import</span> <span class="n">DocList</span><span class="p">,</span> <span class="n">BaseDoc</span>
<span class="kn">from</span> <span class="nn">docarray.typing</span> <span class="kn">import</span> <span class="n">AnyTensor</span>
<span class="kn">from</span> <span class="nn">executor</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>

<span class="k">class</span> <span class="nc">MyDoc</span><span class="p">(</span><span class="n">BaseDoc</span><span class="p">):</span>
    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="n">embedding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AnyTensor</span><span class="p">[</span><span class="mi">5</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">generate_docs</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10_000</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">MyDoc</span><span class="p">(</span>
            <span class="n">text</span><span class="o">=</span><span class="s1">&#39;Using a GPU allows you to significantly speed up encoding.&#39;</span>
        <span class="p">)</span>

<span class="hll"><span class="n">dep</span> <span class="o">=</span> <span class="n">Deployment</span><span class="p">(</span><span class="n">uses</span><span class="o">=</span><span class="s1">&#39;docker://sentence-encoder&#39;</span><span class="p">,</span> <span class="n">uses_with</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">},</span> <span class="n">gpus</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
</span>
<span class="k">with</span> <span class="n">dep</span><span class="p">:</span>
    <span class="n">dep</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">on</span><span class="o">=</span><span class="s1">&#39;/encode&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">generate_docs</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">request_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="n">DocList</span><span class="p">[</span><span class="n">MyDoc</span><span class="p">])</span>
</pre></div>
</div>
<p>If we run this with <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">main.py</span></code>, well get the same output as before, except that now well also get the output from the Docker container.</p>
<p>Every time we start the Executor, the Transformer model gets downloaded again. To speed this up, we want the encoder to load the model from a file which we have pre-downloaded to our disk.</p>
<p>We can do this with Docker volumes - Jina simply passes the argument to the Docker container. Heres how we modify <code class="docutils literal notranslate"><span class="pre">main.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dep</span> <span class="o">=</span> <span class="n">Deployment</span><span class="p">(</span>
    <span class="n">uses</span><span class="o">=</span><span class="s1">&#39;docker://sentence-encoder&#39;</span><span class="p">,</span>
    <span class="n">uses_with</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">},</span>
    <span class="n">gpus</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span>
    <span class="c1"># This has to be an absolute path, replace /home/ubuntu with your home directory</span>
    <span class="n">volumes</span><span class="o">=</span><span class="s2">&quot;/home/ubuntu/.cache:/root/.cache&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We mounted the <code class="docutils literal notranslate"><span class="pre">~/.cache</span></code> directory, because thats where pre-built transformer models are saved. But this could be any custom directory - depending on the Python package you are using, and how you specify the model loading path.</p>
<p>Run <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">main.py</span></code> again and you can see that no downloading happens inside the container, and that encoding starts faster.</p>
</section>
<section id="using-gpu-with-hub-executors">
<h2>Using GPU with Hub Executors<a class="headerlink" href="#using-gpu-with-hub-executors" title="Permalink to this heading">#</a></h2>
<p>We now saw how to use GPU with our Executor locally, and when using it in a Docker container. What about when we use Executors from Executor Hub - is there any difference?</p>
<p>Nope! Not only that, many Executors on Executor Hub already come with a GPU-enabled version pre-built, usually under the <code class="docutils literal notranslate"><span class="pre">gpu</span></code> tag (see <a class="reference internal" href="../../concepts/serving/executor/hub/push-executor/#hub-tags"><span class="std std-ref">Executor Hub tags</span></a>). Lets modify our example to use the pre-built <code class="docutils literal notranslate"><span class="pre">TransformerTorchEncoder</span></code> from Executor Hub:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span>dep = Deployment(
<span class="gd">-   uses=&#39;docker://sentence-encoder&#39;,</span>
<span class="gi">+   uses=&#39;jinaai+docker://jina-ai/TransformerTorchEncoder:latest-gpu&#39;,</span>
<span class="w"> </span>   uses_with={&#39;device&#39;: &#39;cuda&#39;},
<span class="w"> </span>   gpus=&#39;all&#39;,
<span class="w"> </span>   # This has to be an absolute path, replace /home/ubuntu with your home directory
<span class="w"> </span>   volumes=&quot;/home/ubuntu/.cache:/root/.cache&quot;
)
</pre></div>
</div>
<p>The first time you run the script, downloading the Docker image takes some time - GPU images are large! But after that, everything works just as it did with the local Docker image, out of the box.</p>
<div class="caution admonition">
<p class="admonition-title">Important</p>
<p>When using GPU encoders from Executor Hub, always use <code class="docutils literal notranslate"><span class="pre">jinaai+docker://</span></code>, and not <code class="docutils literal notranslate"><span class="pre">jinaai://</span></code>. As discussed above, these encoders may need CUDA installed (or other system dependencies), and installing that properly can be tricky. For that reason, use Docker images, which already come with all these dependencies pre-installed.</p>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>Lets recap this tutorial:</p>
<ol class="arabic simple">
<li><p>Using Executors on a GPU locally is no different to using a GPU in a standalone script. You pass the device you want your Executor to use in the initialization.</p></li>
<li><p>To use an Executor on a GPU inside a Docker container, pass <code class="docutils literal notranslate"><span class="pre">gpus='all'</span></code>.</p></li>
<li><p>Use volumes (bind mounts), so you dont have to download large files each time you start the Executor.</p></li>
<li><p>Use GPU with Executors from Executor Hub - just use the Executor with the <code class="docutils literal notranslate"><span class="pre">gpu</span></code> tag.</p></li>
</ol>
<p>When you start building your own Executor, check what system requirements (CUDA and similar) are needed, and install them locally (and in the <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code>) accordingly.</p>
</section>
</section>

                </article>
            </div>
            <footer>
                
                <div class="related-pages">
                    <a class="next-page" href="../llm-serve/">
                        <div class="page-info">
                            <div class="context">
                                <span>Next</span>
                            </div>
                            <div class="title">Build a Streaming API for a Large Language Model</div>
                        </div>
                        <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
                    </a>
                    <a class="prev-page" href="../deploy-model/">
                        <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
                        <div class="page-info">
                            <div class="context">
                                <span>Previous</span>
                            </div>
                            
                            <div class="title">Deploy a model</div>
                            
                        </div>
                    </a>
                </div>
                <div class="bottom-of-page">
                    <div class="left-details">
                        <div class="copyright">
                            Copyright &#169; Jina AI Limited. All rights reserved.
                        </div><div class="last-updated">
                            Last updated on Sep 20, 2024</div>
                    </div>
                    <div class="right-details">
                        <div class="social-btns">
                            <a class='social-btn' href="https://github.com/jina-ai/jina" aria-label="GitHub"
                               target="_blank" rel="noreferrer"> <i class="fab fa-github"></i></a>
                            <a class='social-btn' href="https://discord.jina.ai" aria-label="Discord" target="_blank"
                               rel="noreferrer"> <i class="fab fa-discord"></i></a>
                            <a class='social-btn' href="https://youtube.com/c/jina-ai" aria-label="YouTube"
                               target="_blank" rel="noreferrer"> <i class="fab fa-youtube"></i></a>
                            <a class='social-btn' href="https://twitter.com/JinaAI_" aria-label="Twitter"
                               target="_blank" rel="noreferrer"> <i class="fab fa-twitter"></i></a>
                            <a class='social-btn' href="https://www.linkedin.com/company/jinaai/" aria-label="LinkedIn"
                               target="_blank" rel="noreferrer"> <i class="fab fa-linkedin"></i></a>
                        </div>
                    </div>
                </div>
                
            </footer>
        </div>
        <aside class="toc-drawer">
            
            
            <div class="toc-sticky toc-scroll">
                <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
                </div>
                <div class="toc-tree-container">
                    <div class="toc-tree">
                        <ul>
<li><a class="reference internal" href="#">Build a GPU Executor</a><ul>
<li><a class="reference internal" href="#jina-and-gpus-in-a-nutshell">Jina and GPUs in a nutshell</a></li>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li><a class="reference internal" href="#setting-up-the-executor">Setting up the Executor</a></li>
<li><a class="reference internal" href="#running-on-gpu-and-cpu-locally">Running on GPU and CPU locally</a></li>
<li><a class="reference internal" href="#using-gpu-in-a-container">Using GPU in a container</a></li>
<li><a class="reference internal" href="#using-gpu-with-hub-executors">Using GPU with Hub Executors</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

                    </div>
                </div>
            </div>
            
            
        </aside>
    </div>
</div>
<img referrerpolicy="no-referrer-when-downgrade"
     src="https://static.scarf.sh/a.png?x-pxid=2823e771-0e1e-4320-8fde-48bc48e53262"/><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    </body>
</html>